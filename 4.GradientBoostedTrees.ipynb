{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import timeit\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "import os\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import math\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from random import random\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = 'C:/Program Files/Java/jre1.8.0_333/'\n",
    "os.environ[\"SPARK_HOME\"] = 'C:/spark/spark-3.3.0-bin-hadoop2/'\n",
    "os.environ[\"HADOOP_HOME\"] = 'C:/hadoop/'\n",
    "#os.environ[\"PYSPARK_SUBMIT_ARGS\"] = '--master local pyspark-shell'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:/spark/spark-3.3.0-bin-hadoop2/'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "findspark.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".master(\"local\")\\\n",
    ".config(\"spark.executor.cores\", \"4\")\\\n",
    ".config(\"spark.executor.memory\", \"10g\")\\\n",
    ".config(\"spark.driver.memory\", \"10g\")\\\n",
    ".config(\"spark.executor.memoryOverhead\", \"10g\")\\\n",
    ".appName(\"Churn Project\").getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df = spark.read.csv('C:/Users/paulc/Documents/Data Analysis Courses/MastersChurn/ProjectWork/ChurnModelScaled.csv',inferSchema=True,header=True)\n",
    "scaled_df = spark.read.csv('C:/Users/paulc/Documents/Data Analysis Courses/MastersChurn/ProjectWork/ChurnModelScaled.csv',inferSchema=True,header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df = df.drop('_c0')\n",
    "scaled_df = scaled_df.drop('_c0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MonthNumber: double (nullable = true)\n",
      " |-- ClientProfileSummary: double (nullable = true)\n",
      " |-- Gender: double (nullable = true)\n",
      " |-- CashActive_YN: double (nullable = true)\n",
      " |-- L1DSBBetCount: double (nullable = true)\n",
      " |-- L7DSBBetCount: double (nullable = true)\n",
      " |-- L30DSBBetCount: double (nullable = true)\n",
      " |-- L90DSBBetCount: double (nullable = true)\n",
      " |-- L1DDepositCount: double (nullable = true)\n",
      " |-- L7DDepositCount: double (nullable = true)\n",
      " |-- L30DDepositCount: double (nullable = true)\n",
      " |-- L90DDepositCount: double (nullable = true)\n",
      " |-- L7DOtherSportsBetCount: double (nullable = true)\n",
      " |-- L30DOtherSportsBetCount: double (nullable = true)\n",
      " |-- L90DOtherSportsBetCount: double (nullable = true)\n",
      " |-- L90DUnsuccessfulDepositCount: double (nullable = true)\n",
      " |-- DaysSinceLastSBCashAPD: double (nullable = true)\n",
      " |-- DaysSinceLastSBAPD: double (nullable = true)\n",
      " |-- OlderMale40: double (nullable = true)\n",
      " |-- CustomerConcession90days: double (nullable = true)\n",
      " |-- Active_Next30Days_Cash_YN: double (nullable = true)\n",
      " |-- L7DSBTurnover: double (nullable = true)\n",
      " |-- US_SportsTurnover7D: double (nullable = true)\n",
      " |-- L30DSBTurnover: double (nullable = true)\n",
      " |-- US_SportsTurnover30D: double (nullable = true)\n",
      " |-- CustomerConcession30days: double (nullable = true)\n",
      " |-- L90DSBTurnover: double (nullable = true)\n",
      " |-- L1DSBTurnover: double (nullable = true)\n",
      " |-- US_SportsTurnover1D: double (nullable = true)\n",
      " |-- L30DSBFreeBetsHandle: double (nullable = true)\n",
      " |-- US_SportsTurnover90D: double (nullable = true)\n",
      " |-- L7DSBFreeBetsHandle: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vector Assembler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "numericCols = ['MonthNumber', 'ClientProfileSummary', 'Gender', 'CashActive_YN',\n",
    "       'L1DSBBetCount', 'L7DSBBetCount', 'L30DSBBetCount', 'L90DSBBetCount',\n",
    "       'L1DDepositCount', 'L7DDepositCount', 'L30DDepositCount',\n",
    "       'L90DDepositCount', 'L7DOtherSportsBetCount', 'L30DOtherSportsBetCount',\n",
    "       'L90DOtherSportsBetCount', 'L90DUnsuccessfulDepositCount',\n",
    "       'DaysSinceLastSBCashAPD', 'DaysSinceLastSBAPD', 'OlderMale40',\n",
    "       'CustomerConcession90days',\n",
    "       'L7DSBTurnover', 'US_SportsTurnover7D', 'L30DSBTurnover',\n",
    "       'US_SportsTurnover30D', 'CustomerConcession30days', 'L90DSBTurnover',\n",
    "       'L1DSBTurnover', 'US_SportsTurnover1D', 'L30DSBFreeBetsHandle',\n",
    "       'US_SportsTurnover90D', 'L7DSBFreeBetsHandle']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "scaled_df = assembler.transform(scaled_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train/Test Split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train,test = df.randomSplit([0.7,0.3],seed=42)\n",
    "train_scaled,test_scaled = scaled_df.randomSplit([0.7,0.3],seed=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Run a base Gradient Boosted Tree to benchmark\n",
    "\n",
    "\n",
    "The implementation is based upon: J.H. Friedman. “Stochastic Gradient Boosting.” 1999.\n",
    "Soon after the introduction of gradient boosting, Friedman proposed a minor modification to the algorithm, motivated by Breiman's bootstrap aggregation (\"bagging\") method.[6] Specifically, he proposed that at each iteration of the algorithm, a base learner should be fit on a subsample of the training set drawn at random without replacement.[12] Friedman observed a substantial improvement in gradient boosting's accuracy with this modification."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(featuresCol='features',labelCol='Active_Next30Days_Cash_YN',maxIter=20,seed=42,featureSubsetStrategy='sqrt')\n",
    "\n",
    "\n",
    "gbtModel = gbt.fit(train)\n",
    "predictions = gbtModel.transform(test)\n",
    "#predictions.select('prediction','Active_Next30Days_Cash_YN', 'probability')\n",
    "\n",
    "\n",
    "#Scaled model\n",
    "\n",
    "gbt_scaled = GBTClassifier(featuresCol='features',labelCol='Active_Next30Days_Cash_YN',maxIter=20,seed=42,featureSubsetStrategy='sqrt')\n",
    "\n",
    "\n",
    "gbtModel_scaled = gbt_scaled.fit(train_scaled)\n",
    "predictions_scaled = gbtModel_scaled.transform(test_scaled)\n",
    "#predictions_scaled.select('prediction','Active_Next30Days_Cash_YN', 'probability')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base GradientBoostedClassifier Test error is: 0.1678\n",
      "The base ScaledGradientBoostedClassifier Test error is: 0.1678\n"
     ]
    }
   ],
   "source": [
    "evaluation = MulticlassClassificationEvaluator(labelCol='Active_Next30Days_Cash_YN', predictionCol='prediction',metricName='accuracy')\n",
    "accuracy = evaluation.evaluate(predictions)\n",
    "print(\"The base GradientBoostedClassifier Test error is:\", round((1.0 - accuracy),4))\n",
    "\n",
    "evaluation_scaled = MulticlassClassificationEvaluator(labelCol='Active_Next30Days_Cash_YN', predictionCol='prediction',metricName='accuracy')\n",
    "accuracy_scaled = evaluation_scaled.evaluate(predictions_scaled)\n",
    "print(\"The base ScaledGradientBoostedClassifier Test error is:\", round((1.0 - accuracy_scaled),4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of the base GradientBoostedClassifier is: 0.8322\n",
      "The Accuracy of the base ScaledGradientBoostedClassifier is: 0.8322\n",
      "Improvement as a result of scaling is: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"The Accuracy of the base GradientBoostedClassifier is:\", round(accuracy,4))\n",
    "print(\"The Accuracy of the base ScaledGradientBoostedClassifier is:\", round(accuracy_scaled,4))\n",
    "print(\"Improvement as a result of scaling is:\", round(0.8354-0.8354,4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "lit() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [25]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# replicate the spark dataframe into multiple copies\u001B[39;00m\n\u001B[0;32m      5\u001B[0m w \u001B[38;5;241m=\u001B[39m Window()\u001B[38;5;241m.\u001B[39morderBy()\n\u001B[1;32m----> 6\u001B[0m replication_df \u001B[38;5;241m=\u001B[39m train\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreplication_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[43mlit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1001\u001B[39;49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mTypeError\u001B[0m: lit() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, lit\n",
    "\n",
    "# replicate the spark dataframe into multiple copies\n",
    "w = Window().orderBy()\n",
    "replication_df = train.withColumn('replication_id', lit())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[MonthNumber: double, ClientProfileSummary: double, Gender: double, CashActive_YN: double, L1DSBBetCount: double, L7DSBBetCount: double, L30DSBBetCount: double, L90DSBBetCount: double, L1DDepositCount: double, L7DDepositCount: double, L30DDepositCount: double, L90DDepositCount: double, L7DOtherSportsBetCount: double, L30DOtherSportsBetCount: double, L90DOtherSportsBetCount: double, L90DUnsuccessfulDepositCount: double, DaysSinceLastSBCashAPD: double, DaysSinceLastSBAPD: double, OlderMale40: double, CustomerConcession90days: double, Active_Next30Days_Cash_YN: double, L7DSBTurnover: double, US_SportsTurnover7D: double, L30DSBTurnover: double, US_SportsTurnover30D: double, CustomerConcession30days: double, L90DSBTurnover: double, L1DSBTurnover: double, US_SportsTurnover1D: double, L30DSBFreeBetsHandle: double, US_SportsTurnover90D: double, L7DSBFreeBetsHandle: double, features: vector, replication_id: int]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replication_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Randomized Hyperparameter search\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class RandomGridBuilder:\n",
    "  '''Grid builder for random search. Sets up grids for use in CrossValidator in Spark using values randomly sampled from user-provided distributions.\n",
    "  Distributions should be provided as lambda functions, so that the numbers are generated at call time.\n",
    "\n",
    "  Parameters:\n",
    "    num_models: Integer (Python) - number of models to generate hyperparameters for\n",
    "    seed: Integer (Python) - seed (optional, default is None)\n",
    "\n",
    "  Returns:\n",
    "    param_map: list of parameter maps to use in cross validation.\n",
    "\n",
    "  Example usage:\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    lr = LogisticRegression()\n",
    "    paramGrid = RandomGridBuilder(2)\\\n",
    "               .addDistr(lr.regParam, lambda: np.random.rand()) \\\n",
    "               .addDistr(lr.maxIter, lambda : np.random.randint(10))\\\n",
    "               .build()\n",
    "\n",
    "    Returns similar output as Spark ML class ParamGridBuilder and can be used in its place. The above paramGrid provides random hyperparameters for 2 models.\n",
    "    '''\n",
    "\n",
    "  def __init__(self, num_models, seed=None):\n",
    "    self._param_grid = {}\n",
    "    self.num_models = num_models\n",
    "    self.seed = seed\n",
    "\n",
    "  def addDistr(self, param, distr_generator):\n",
    "    '''Add distribution based on dictionary generated by function passed to addDistr.'''\n",
    "\n",
    "    if 'pyspark.ml.param.Param' in str(type(param)):\n",
    "      self._param_grid[param] = distr_generator\n",
    "    else:\n",
    "      raise TypeError('param must be an instance of Param')\n",
    "\n",
    "    return self\n",
    "\n",
    "  def build(self):\n",
    "    param_map = []\n",
    "    for n in range(self.num_models):\n",
    "      if self.seed:\n",
    "        # Set seeds for both numpy and random in case either is used for the random distribution\n",
    "        np.random.seed(self.seed + n)\n",
    "        random.seed(self.seed + n)\n",
    "      param_dict = {}\n",
    "      for param, distr in self._param_grid.items():\n",
    "        param_dict[param] = distr()\n",
    "      param_map.append(param_dict)\n",
    "\n",
    "    return param_map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "gbtRS = GBTClassifier(featuresCol='features',labelCol='Active_Next30Days_Cash_YN',featureSubsetStrategy='sqrt')\n",
    "\n",
    "# Set up cross validation with random search\n",
    "randomParams = RandomGridBuilder(num_models=1)\\\n",
    "                 .addDistr(gbt.maxIter, lambda : np.random.randint(10,40))\\\n",
    "                 .addDistr(gbt.maxDepth, lambda : np.random.randint(5,30))\\\n",
    "                 .build()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [33]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Cross validator with random search\u001B[39;00m\n\u001B[0;32m      4\u001B[0m cv \u001B[38;5;241m=\u001B[39m CrossValidator(\n\u001B[0;32m      5\u001B[0m                     estimator \u001B[38;5;241m=\u001B[39m gbtRS,\n\u001B[0;32m      6\u001B[0m                     estimatorParamMaps \u001B[38;5;241m=\u001B[39m randomParams,\n\u001B[0;32m      7\u001B[0m                     evaluator \u001B[38;5;241m=\u001B[39m evaluator,\n\u001B[0;32m      8\u001B[0m                     parallelism \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m      9\u001B[0m                    )\n\u001B[1;32m---> 11\u001B[0m cvModel \u001B[38;5;241m=\u001B[39m \u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\spark\\spark-3.3.0-bin-hadoop2\\python\\pyspark\\ml\\base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[1;34m(self, dataset, params)\u001B[0m\n\u001B[0;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[0;32m    210\u001B[0m     )\n",
      "File \u001B[1;32mC:\\spark\\spark-3.3.0-bin-hadoop2\\python\\pyspark\\ml\\tuning.py:862\u001B[0m, in \u001B[0;36mCrossValidator._fit\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m    860\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    861\u001B[0m     bestIndex \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmin(metrics)\n\u001B[1;32m--> 862\u001B[0m bestModel \u001B[38;5;241m=\u001B[39m \u001B[43mest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepm\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbestIndex\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    863\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(\n\u001B[0;32m    864\u001B[0m     CrossValidatorModel(bestModel, metrics, cast(List[List[Model]], subModels), std_metrics)\n\u001B[0;32m    865\u001B[0m )\n",
      "File \u001B[1;32mC:\\spark\\spark-3.3.0-bin-hadoop2\\python\\pyspark\\ml\\base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[1;34m(self, dataset, params)\u001B[0m\n\u001B[0;32m    201\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(params, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m    202\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m params:\n\u001B[1;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
      "File \u001B[1;32mC:\\spark\\spark-3.3.0-bin-hadoop2\\python\\pyspark\\ml\\wrapper.py:379\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m    378\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n\u001B[1;32m--> 379\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    380\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n\u001B[0;32m    381\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
      "File \u001B[1;32mC:\\spark\\spark-3.3.0-bin-hadoop2\\python\\pyspark\\ml\\wrapper.py:376\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m    373\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    375\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[1;32m--> 376\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1303\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1296\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m   1298\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1299\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1300\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1301\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m-> 1303\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1304\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[0;32m   1305\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[0;32m   1307\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1033\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[1;34m(self, command, retry, binary)\u001B[0m\n\u001B[0;32m   1031\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[0;32m   1032\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1033\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1034\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[0;32m   1035\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\clientserver.py:475\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[1;34m(self, command)\u001B[0m\n\u001B[0;32m    473\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    474\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 475\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m    476\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[0;32m    477\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[0;32m    478\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\socket.py:704\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    703\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 704\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    706\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol='Active_Next30Days_Cash_YN', predictionCol='prediction',metricName='accuracy')\n",
    "\n",
    "# Cross validator with random search\n",
    "cv = CrossValidator(\n",
    "                    estimator = gbtRS,\n",
    "                    estimatorParamMaps = randomParams,\n",
    "                    evaluator = evaluator,\n",
    "                    parallelism = 10\n",
    "                   )\n",
    "\n",
    "cvModel = cv.fit(train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cvModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [40]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mcvModel\u001B[49m\u001B[38;5;241m.\u001B[39mbestModel\n",
      "\u001B[1;31mNameError\u001B[0m: name 'cvModel' is not defined"
     ]
    }
   ],
   "source": [
    "cvModel.bestModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "gbt = GBTClassifier(featuresCol='features',labelCol='Active_Next30Days_Cash_YN',featureSubsetStrategy='sqrt')\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxDepth, [10, 30])\\\n",
    "                              .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}